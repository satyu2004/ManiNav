{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sathya\\AppData\\Local\\Temp\\ipykernel_24632\\2889638705.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X0, V, pos = torch.load(f\"{path}/X0.pt\"), torch.load(f\"{path}/V.pt\"), torch.load(f\"{path}/pos.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = 'data'\n",
    "\n",
    "# X0_1 = torch.load(f\"{path}/X0_1.pt\")\n",
    "# X0_2 = torch.load(f\"{path}/X0_2.pt\")\n",
    "# V_1 = torch.load(f\"{path}/V_1.pt\")\n",
    "# V_2 = torch.load(f\"{path}/V_2.pt\")\n",
    "# pos_1 = torch.load(f\"{path}/pos_1.pt\")\n",
    "# pos_2 = torch.load(f\"{path}/pos_2.pt\")\n",
    "\n",
    "# X0 = torch.cat((X0_1, X0_2), dim=0)\n",
    "# V = torch.cat((V_1, V_2), dim=0)\n",
    "# pos = torch.cat((pos_1, pos_2), dim=0)\n",
    "\n",
    "# torch.save(X0, f\"{path}/X0.pt\")\n",
    "# torch.save(V, f\"{path}/V.pt\")\n",
    "# torch.save(pos, f\"{path}/pos.pt\")\n",
    "\n",
    "X0, V, pos = torch.load(f\"{path}/X0.pt\"), torch.load(f\"{path}/V.pt\"), torch.load(f\"{path}/pos.pt\")\n",
    "X0.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x151fff4cb90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X0[:, 0], X0[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sathya\\AppData\\Local\\Temp\\ipykernel_24632\\3325782543.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  f.load_state_dict(torch.load('neural_surface.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0459,  0.0623, -0.0376],\n",
       "         [-0.0413,  0.0057,  0.0183],\n",
       "         [-0.0039, -0.0073,  0.0067],\n",
       "         [-0.0256, -0.0955,  0.0102],\n",
       "         [-0.0392, -0.0073,  0.0575],\n",
       "         [-0.0108,  0.0166, -0.0264],\n",
       "         [-0.0262, -0.0044,  0.0114],\n",
       "         [-0.0075,  0.0671, -0.0268],\n",
       "         [ 0.0312, -0.0162,  0.0554],\n",
       "         [ 0.0125,  0.0686, -0.0450],\n",
       "         [ 0.0423, -0.0812,  0.0676],\n",
       "         [ 0.0109,  0.0375,  0.0105],\n",
       "         [ 0.0106,  0.0633, -0.0033],\n",
       "         [ 0.0060,  0.0864, -0.0539],\n",
       "         [ 0.0291,  0.0377, -0.0699],\n",
       "         [-0.0413, -0.0427,  0.0423],\n",
       "         [ 0.0161,  0.0578,  0.0153],\n",
       "         [-0.0346,  0.0617, -0.0156],\n",
       "         [-0.0665,  0.0177, -0.0130],\n",
       "         [ 0.0266,  0.0517, -0.0485],\n",
       "         [ 0.0274, -0.0153,  0.0392],\n",
       "         [-0.0472,  0.0060, -0.0024],\n",
       "         [-0.0798,  0.0314,  0.0285],\n",
       "         [-0.0600,  0.0409, -0.0672],\n",
       "         [-0.0275, -0.0292,  0.0171],\n",
       "         [ 0.0116, -0.0164,  0.0354],\n",
       "         [ 0.0416,  0.0135, -0.0329],\n",
       "         [ 0.0852, -0.0068, -0.0269],\n",
       "         [ 0.0101, -0.0194,  0.0107],\n",
       "         [ 0.0472, -0.0583,  0.0489],\n",
       "         [-0.0094,  0.0500, -0.0523],\n",
       "         [ 0.0316,  0.0761, -0.0517],\n",
       "         [ 0.0832, -0.0301, -0.0023],\n",
       "         [ 0.0774,  0.0121, -0.0329],\n",
       "         [ 0.0453,  0.0271, -0.0495],\n",
       "         [-0.0625,  0.0165,  0.0349],\n",
       "         [-0.0025,  0.0002,  0.0003],\n",
       "         [-0.0521,  0.0488, -0.0671],\n",
       "         [-0.0056, -0.0221, -0.0118],\n",
       "         [ 0.0562, -0.0457,  0.0468],\n",
       "         [ 0.0387,  0.0096, -0.0436],\n",
       "         [ 0.0235, -0.0039, -0.0147],\n",
       "         [ 0.0412,  0.0266, -0.0290],\n",
       "         [ 0.0018,  0.0321, -0.0323],\n",
       "         [-0.0381,  0.0367, -0.0360],\n",
       "         [ 0.0684,  0.0259, -0.0427],\n",
       "         [ 0.0181, -0.0106, -0.0111],\n",
       "         [ 0.0515, -0.0122, -0.0228],\n",
       "         [-0.0301, -0.0342, -0.0373],\n",
       "         [-0.0550,  0.0119,  0.0021],\n",
       "         [ 0.0220,  0.0448, -0.0334],\n",
       "         [-0.0328, -0.0539,  0.0481],\n",
       "         [ 0.0362, -0.0764,  0.0507],\n",
       "         [-0.0695,  0.0636,  0.0236],\n",
       "         [ 0.0178, -0.0299,  0.0703],\n",
       "         [-0.0109, -0.0133,  0.0124],\n",
       "         [ 0.0760, -0.0088, -0.0267],\n",
       "         [ 0.0135, -0.0484,  0.0015],\n",
       "         [ 0.0539,  0.0029, -0.0328],\n",
       "         [ 0.0867, -0.0018, -0.0281],\n",
       "         [ 0.0685, -0.0582,  0.0243],\n",
       "         [ 0.0256, -0.0480, -0.0079],\n",
       "         [ 0.0032, -0.0720,  0.0161],\n",
       "         [ 0.0587, -0.0337,  0.0010],\n",
       "         [ 0.0115,  0.0074,  0.0108],\n",
       "         [-0.0903,  0.0125, -0.0121],\n",
       "         [-0.0067,  0.0357, -0.0093],\n",
       "         [-0.0435,  0.0313, -0.0405],\n",
       "         [ 0.0439,  0.0484, -0.0489],\n",
       "         [-0.0636, -0.0257,  0.0866],\n",
       "         [ 0.0166,  0.0917, -0.0337],\n",
       "         [-0.0271,  0.0580, -0.0134],\n",
       "         [-0.0487,  0.0262, -0.0324],\n",
       "         [-0.0221,  0.0146, -0.0387],\n",
       "         [-0.0734,  0.0038,  0.0085],\n",
       "         [ 0.0655, -0.0187, -0.0516],\n",
       "         [ 0.0313,  0.0594, -0.0559],\n",
       "         [-0.0238, -0.0422,  0.0385],\n",
       "         [ 0.0058, -0.0587,  0.0684],\n",
       "         [ 0.0343, -0.0404,  0.0132],\n",
       "         [-0.0132, -0.0296,  0.0194],\n",
       "         [-0.0766,  0.0108,  0.0093],\n",
       "         [ 0.0073,  0.0093, -0.0090],\n",
       "         [ 0.0019,  0.0061, -0.0089],\n",
       "         [ 0.0486, -0.0734,  0.0616],\n",
       "         [ 0.0251,  0.0410,  0.0157],\n",
       "         [-0.0553, -0.0552,  0.0595],\n",
       "         [-0.0252,  0.0796, -0.0245],\n",
       "         [-0.0128, -0.0108,  0.0101],\n",
       "         [-0.0073, -0.0018,  0.0117],\n",
       "         [ 0.0261, -0.0666,  0.0453],\n",
       "         [-0.0123, -0.0933, -0.0188],\n",
       "         [ 0.0710, -0.0169, -0.0173],\n",
       "         [ 0.0353, -0.0627,  0.0762],\n",
       "         [ 0.0659,  0.0030, -0.0352],\n",
       "         [-0.0334,  0.0402, -0.0114],\n",
       "         [ 0.0042,  0.0857, -0.0617],\n",
       "         [-0.0610,  0.0388, -0.0138],\n",
       "         [ 0.0446,  0.0600, -0.0379],\n",
       "         [-0.0101, -0.0039,  0.0049]], grad_fn=<SubBackward0>),\n",
       " tensor([[ 0.0459,  0.0623, -0.0425],\n",
       "         [-0.0413,  0.0057,  0.0123],\n",
       "         [-0.0039, -0.0073,  0.0067],\n",
       "         [-0.0256, -0.0955,  0.0018],\n",
       "         [-0.0392, -0.0073,  0.0744],\n",
       "         [-0.0108,  0.0166, -0.0292],\n",
       "         [-0.0262, -0.0044,  0.0114],\n",
       "         [-0.0075,  0.0671, -0.0452],\n",
       "         [ 0.0312, -0.0162,  0.0548],\n",
       "         [ 0.0125,  0.0686, -0.0501],\n",
       "         [ 0.0423, -0.0812,  0.0352],\n",
       "         [ 0.0109,  0.0375,  0.0144],\n",
       "         [ 0.0106,  0.0633,  0.0064],\n",
       "         [ 0.0060,  0.0864, -0.0474],\n",
       "         [ 0.0291,  0.0377, -0.0843],\n",
       "         [-0.0413, -0.0427,  0.0428],\n",
       "         [ 0.0161,  0.0578,  0.0203],\n",
       "         [-0.0346,  0.0617, -0.0387],\n",
       "         [-0.0665,  0.0177, -0.0196],\n",
       "         [ 0.0266,  0.0517, -0.0556],\n",
       "         [ 0.0274, -0.0153,  0.0422],\n",
       "         [-0.0472,  0.0060,  0.0005],\n",
       "         [-0.0798,  0.0314, -0.0088],\n",
       "         [-0.0600,  0.0409, -0.0451],\n",
       "         [-0.0275, -0.0292,  0.0152],\n",
       "         [ 0.0116, -0.0164,  0.0324],\n",
       "         [ 0.0416,  0.0135, -0.0346],\n",
       "         [ 0.0852, -0.0068, -0.0110],\n",
       "         [ 0.0101, -0.0194,  0.0127],\n",
       "         [ 0.0472, -0.0583,  0.0280],\n",
       "         [-0.0094,  0.0500, -0.0408],\n",
       "         [ 0.0316,  0.0761, -0.0361],\n",
       "         [ 0.0832, -0.0301,  0.0392],\n",
       "         [ 0.0774,  0.0121, -0.0257],\n",
       "         [ 0.0453,  0.0271, -0.0411],\n",
       "         [-0.0625,  0.0165,  0.0342],\n",
       "         [-0.0025,  0.0002,  0.0003],\n",
       "         [-0.0521,  0.0488, -0.0610],\n",
       "         [-0.0056, -0.0221, -0.0114],\n",
       "         [ 0.0562, -0.0457,  0.0319],\n",
       "         [ 0.0387,  0.0096, -0.0489],\n",
       "         [ 0.0235, -0.0039, -0.0181],\n",
       "         [ 0.0412,  0.0266, -0.0304],\n",
       "         [ 0.0018,  0.0321, -0.0346],\n",
       "         [-0.0381,  0.0367, -0.0374],\n",
       "         [ 0.0684,  0.0259, -0.0482],\n",
       "         [ 0.0181, -0.0106, -0.0086],\n",
       "         [ 0.0515, -0.0122, -0.0083],\n",
       "         [-0.0301, -0.0342, -0.0496],\n",
       "         [-0.0550,  0.0119,  0.0133],\n",
       "         [ 0.0220,  0.0448, -0.0312],\n",
       "         [-0.0328, -0.0539,  0.0491],\n",
       "         [ 0.0362, -0.0764,  0.0312],\n",
       "         [-0.0695,  0.0636, -0.0088],\n",
       "         [ 0.0178, -0.0299,  0.0656],\n",
       "         [-0.0109, -0.0133,  0.0123],\n",
       "         [ 0.0760, -0.0088, -0.0245],\n",
       "         [ 0.0135, -0.0484,  0.0511],\n",
       "         [ 0.0539,  0.0029, -0.0448],\n",
       "         [ 0.0867, -0.0018, -0.0102],\n",
       "         [ 0.0685, -0.0582,  0.0378],\n",
       "         [ 0.0256, -0.0480, -0.0110],\n",
       "         [ 0.0032, -0.0720,  0.0199],\n",
       "         [ 0.0587, -0.0337, -0.0245],\n",
       "         [ 0.0115,  0.0074,  0.0102],\n",
       "         [-0.0903,  0.0125,  0.0353],\n",
       "         [-0.0067,  0.0357, -0.0131],\n",
       "         [-0.0435,  0.0313, -0.0514],\n",
       "         [ 0.0439,  0.0484, -0.0482],\n",
       "         [-0.0636, -0.0257,  0.0651],\n",
       "         [ 0.0166,  0.0917, -0.0162],\n",
       "         [-0.0271,  0.0580, -0.0044],\n",
       "         [-0.0487,  0.0262, -0.0348],\n",
       "         [-0.0221,  0.0146, -0.0372],\n",
       "         [-0.0734,  0.0038, -0.0093],\n",
       "         [ 0.0655, -0.0187, -0.0319],\n",
       "         [ 0.0313,  0.0594, -0.0528],\n",
       "         [-0.0238, -0.0422,  0.0398],\n",
       "         [ 0.0058, -0.0587,  0.0430],\n",
       "         [ 0.0343, -0.0404,  0.0222],\n",
       "         [-0.0132, -0.0296,  0.0190],\n",
       "         [-0.0766,  0.0108,  0.0210],\n",
       "         [ 0.0073,  0.0093, -0.0092],\n",
       "         [ 0.0019,  0.0061, -0.0090],\n",
       "         [ 0.0486, -0.0734,  0.0325],\n",
       "         [ 0.0251,  0.0410,  0.0046],\n",
       "         [-0.0553, -0.0552,  0.0617],\n",
       "         [-0.0252,  0.0796, -0.0231],\n",
       "         [-0.0128, -0.0108,  0.0124],\n",
       "         [-0.0073, -0.0018,  0.0121],\n",
       "         [ 0.0261, -0.0666,  0.0572],\n",
       "         [-0.0123, -0.0933, -0.0050],\n",
       "         [ 0.0710, -0.0169,  0.0186],\n",
       "         [ 0.0353, -0.0627,  0.0687],\n",
       "         [ 0.0659,  0.0030, -0.0470],\n",
       "         [-0.0334,  0.0402, -0.0267],\n",
       "         [ 0.0042,  0.0857, -0.0112],\n",
       "         [-0.0610,  0.0388, -0.0432],\n",
       "         [ 0.0446,  0.0600, -0.0438],\n",
       "         [-0.0101, -0.0039,  0.0050]], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "from geodesic_solver import Immersed_Manifold\n",
    "\n",
    "def model_tanh(hidden_dims: list):\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(2, hidden_dims[0]),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(hidden_dims[1], 1)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "f = model_tanh([32, 32])\n",
    "# f.load_state_dict(torch.load('Recon_Surface\\\\neural_surface.pth'))\n",
    "f.load_state_dict(torch.load('neural_surface.pth'))\n",
    "\n",
    "class Surface:\n",
    "    # def __init__(self, a, c):\n",
    "    #     self.a = a\n",
    "    #     self.c = c\n",
    "    def immersion(self, point):\n",
    "        \"\"\"Immersion. Input is a batch of 2-tupes in the unit square.\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        X = point[..., 0]\n",
    "        Y = point[..., 1]\n",
    "        Z = f(point).squeeze(-1)  \n",
    "\n",
    "        return torch.stack(\n",
    "            [X, Y, Z],\n",
    "            axis=-1,\n",
    "        ).to(device)\n",
    "\n",
    "    def jacobian_matrix_batch(self, pts):\n",
    "\n",
    "            # Ensure we're on the right device\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            pts = pts.to(device)\n",
    "            batch_size = pts.shape[0]\n",
    "            \n",
    "            # Create the Jacobian tensor\n",
    "            jacobian = torch.zeros(batch_size, 3, 2, device=device)\n",
    "            \n",
    "            # Set the known derivatives for X and Y components\n",
    "            # For the X component (immersed_pts[:, 0]):\n",
    "            jacobian[:, 0, 0] = 1.0  # ∂X/∂x = 1\n",
    "            jacobian[:, 0, 1] = 0.0  # ∂X/∂y = 0\n",
    "            \n",
    "            # For the Y component (immersed_pts[:, 1]):\n",
    "            jacobian[:, 1, 0] = 0.0  # ∂Y/∂x = 0\n",
    "            jacobian[:, 1, 1] = 1.0  # ∂Y/∂y = 1\n",
    "            \n",
    "            # Only compute gradients for the Z component\n",
    "            # Clone and set requires_grad to avoid modifying the input tensor\n",
    "            pts_grad = pts.detach().clone().requires_grad_(True)\n",
    "            \n",
    "            # Compute only the Z component\n",
    "            z_values = f(pts_grad).squeeze()\n",
    "            \n",
    "            # Compute gradients of Z with respect to inputs\n",
    "            z_grad = torch.autograd.grad(\n",
    "                outputs=z_values,\n",
    "                inputs=pts_grad,\n",
    "                grad_outputs=torch.ones_like(z_values, device=device),\n",
    "                create_graph=True,\n",
    "                only_inputs=True\n",
    "            )[0]\n",
    "            \n",
    "            # Store Z gradients in the Jacobian tensor\n",
    "            jacobian[:, 2, :] = z_grad\n",
    "            \n",
    "            return jacobian\n",
    "\n",
    "    def exp(self, base_pts, velocities):\n",
    "        \"Computes exponential maps\"\n",
    "        immersed_manifold = Immersed_Manifold(f=f)\n",
    "        return immersed_manifold.exp(base_pts, velocities)\n",
    "\n",
    "\n",
    " \n",
    "surface = Surface()\n",
    "\n",
    "pos_3d = surface.immersion(pos)\n",
    "jacobian_matrices = surface.jacobian_matrix_batch(pos[:,0])\n",
    "# jacobian_matrices.shape, V[:,0].unsqueeze(dim=-1).shape\n",
    "V_3d = torch.bmm(jacobian_matrices, V[:,1].unsqueeze(dim=-1)).squeeze()\n",
    "diff = pos_3d[:,1]-pos_3d[:,0]\n",
    "diff, V_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0364,  0.0161,  0.0114],\n",
       "         [-0.0262, -0.0295,  0.0349],\n",
       "         [ 0.0242,  0.0933, -0.0192],\n",
       "         ...,\n",
       "         [ 0.0375, -0.0345,  0.0023],\n",
       "         [ 0.0341,  0.0141,  0.0056],\n",
       "         [ 0.0575, -0.0836, -0.0143]], grad_fn=<SubBackward0>),\n",
       " tensor([[-0.0367,  0.0156, -0.0106],\n",
       "         [-0.0274, -0.0307, -0.0345],\n",
       "         [ 0.0285,  0.0886,  0.0231],\n",
       "         ...,\n",
       "         [ 0.0377, -0.0342, -0.0036],\n",
       "         [ 0.0346,  0.0143, -0.0057],\n",
       "         [ 0.0546, -0.0775,  0.0149]], grad_fn=<SqueezeBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torus_math import Torus\n",
    "torus = Torus(a=1, c=4)\n",
    "pos_3d = torus.immersion(pos[:,0])\n",
    "jacobian_matrices = torus.jacobian_matrix_batch(X0)\n",
    "jacobian_matrices.shape, V[:,0].unsqueeze(dim=-1).shape\n",
    "V_3d = torch.bmm(jacobian_matrices, V[:,0].unsqueeze(dim=-1)).squeeze()\n",
    "diff = pos_3d-torus.immersion(X0)\n",
    "diff, V_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean displacement tensor([0.0049, 0.0557], grad_fn=<SelectBackward0>)\n",
      "Velocities tensor([0.0054, 0.0557], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "i=3\n",
    "print(f'Euclidean displacement {pos[0].diff(dim=0)[i]}')\n",
    "print(f'Velocities {V[0,i+1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.2316, -4.7206,  0.0000],\n",
       "         [-0.4622,  0.1206,  0.8786]], grad_fn=<SelectBackward0>),\n",
       " tensor([[-1.2316, -0.4622],\n",
       "         [-4.7206,  0.1206],\n",
       "         [ 0.0000,  0.8786]], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torus_math\n",
    "from geodesic_solver import Immersed_Manifold\n",
    "\n",
    "torus = torus_math.Torus(a=1, c=4)\n",
    "jm_torus_math = torus.jacobian_matrix_batch(X0)\n",
    "\n",
    "manifold = Immersed_Manifold(immersion=torus.immersion)\n",
    "jm_autograd = manifold.compute_partial_derivatives(X0).permute(dims=(0,2,1))\n",
    "\n",
    "i=1\n",
    "(jm_torus_math[i], jm_autograd[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3, 2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jm_torus_math[0].shape, torch.linalg.pinv(jm_torus_math[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List unpickled from runtimes_dim_8.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14872288703918457,\n",
       " 0.09399938583374023,\n",
       " 0.09020471572875977,\n",
       " 0.08251452445983887,\n",
       " 0.13127493858337402,\n",
       " 0.09063410758972168,\n",
       " 0.0845937728881836,\n",
       " 0.10508394241333008,\n",
       " 0.08269214630126953,\n",
       " 0.09990859031677246]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "def unpickle_list(filename):\n",
    "    \"\"\"\n",
    "    Unpickles a list from a file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The filename to load the pickled list from.\n",
    "\n",
    "    Returns:\n",
    "        list or None: The unpickled list, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:  # 'rb' for read binary\n",
    "            loaded_list = pickle.load(f)\n",
    "        print(f\"List unpickled from {filename}\")\n",
    "        return loaded_list\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filename}\")\n",
    "        return None\n",
    "    except (IOError, pickle.UnpicklingError) as e: #Handles more possible errors\n",
    "        print(f\"Error unpickling list: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "unpickle_list('runtimes_dim_8.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
