# Model settings

n_layer: 2
n_embd: 128
# No attention heads for Mamba
# Mamba-specific optional:
d_state: 16
d_conv: 4
expand: 2



num_x_tokens: 1
x_dim: 2
v_dim: 2
y_dim: 2

# Training settings
batch_size: 256
num_epochs: 1000
learning_rate: 5e-5
logging_steps: 10
save_strategy: "epoch"
output_dir: "./mamba_checkpoints"
logging_dir: "./mamba_logs"
lr_scheduler_type: "cosine"
weight_decay: .01
eval_every: 1
early_stopping_patience: 20

# Data settings
data_path: "./data"      # Path to folder with X0.pt, V.pt, pos.pt
n_trajectories: 10000     # Max number of examples to load
n_eval: 1000
n_v_steps: 10