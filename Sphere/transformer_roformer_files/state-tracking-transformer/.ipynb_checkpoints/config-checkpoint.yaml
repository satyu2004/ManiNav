# Model settings
model_name: null
# Custom GPT2 setup
n_layer: 2
n_head: 4
n_embd: 128
vocab_size: 1000 # This doesn't do anything, it just needs to be written


num_x_tokens: 3
x_dim: 2
v_dim: 2
y_dim: 2

# Training settings
batch_size: 256
num_epochs: 1000
learning_rate: 5e-5
logging_steps: 10
save_strategy: "epoch"
output_dir: "./checkpoints"
logging_dir: "./logs"
lr_scheduler_type: "cosine"
weight_decay: .01

# Data settings
data_path: "./data"      # Path to folder with X0.pt, V.pt, pos.pt
n_trajectories: 40000     # Max number of examples to load
n_velocity_steps: 80